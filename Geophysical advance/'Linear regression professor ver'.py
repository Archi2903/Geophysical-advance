'Linear regression'
'Statistical aspects of leaat squares regression'
'Least squares regression is a method to find the best-fitting line through a set of points in a scatter plot.'

'блок настроек и импорта библиотек'
import numpy as np # для работы с массивами и матрицами
import matplotlib.pyplot as plt # для построения графиков
from sklearn.linear_model import LinearRegression # Используется для построения линейной регрессионной модели
from sklearn.preprocessing import PolynomialFeatures # для полиномиальной регрессии
 
import matplotlib.font_manager as fm # для работы с шрифтами
import matplotlib as mpl # для работы с графиками основной модуль, позволяет задавать глобальные настройки оформления графиков.

# 예: Windows 기본 한글 글꼴 중 하나 설정 (Malgun Gothic)
plt.rcParams['font.family'] = 'Malgun Gothic' #Установка шрифта Malgun Gothic как основного шрифта для всех графиков.
# Обеспечивает корректное отображение корейских символов на графиках (в противном случае могут отображаться квадратики или знаки вопроса).

# 음수 기호 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False #Устанавливает параметр, который позволяет корректно отображать минус («−») в осях графиках, даже если используется шрифт, который не поддерживает этот символ.
'________________________________________________________________________________________'
"Example 2.1"
'DATA/ Уравнения описывающю.Кривую/ Эталонные знач'
# Данные для линейной регрессии
t = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Входные данные, преобразованный в двумерный массив (через reshape)
# В машинном обучении большинство моделей требуют, чтобы входные данные были в виде двумерного массива — даже если есть только один признак.
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0]) # y = m1+ m2t -0.5m3t^2 Уравнения описывающю.Кривую
# При условии что mtrue=[10m,100m/s,9.8m/s^2]^T Эталонные знач

'Создание полиномиальных признаков (второй степени) расчет и вывод matrix G'
'Полиномы polynomial - необходима,чтобы модель могла описывать нелинейные зависимости между входом и выходом'
poly = PolynomialFeatures(degree=2) 
# Создаётся объект poly класса PolynomialFeatures с параметром degree=2, что означает, что мы хотим создать полиномиальные признаки второй степени.
# например, если у нас есть признак x, то полиномиальные признаки будут x и x^2.
# В результате мы получим два новых признака: x и x^2, которые будут использоваться в модели линейной регрессии.
t_poly = poly.fit_transform(t)
# fit-анализирует структуру данных (хотя тут это формальность) transform-преобразует входные данные t в полиномиальные признаки.
# В результате t_poly будет содержать два столбца: первый столбец - это t, а второй - это t^2.
'Проверка формулы полинома где G=[1, t, -1/2*t^2]'
t_poly_check = t_poly.copy() # Создаём копию t_poly, чтобы не изменять оригинальный массив.
t_poly_check[:, 2] *= -0.5 # Умножаем третий столбец (t^2) на -0.5, чтобы получить формулу G=[1, t, -1/2*t^2].
t_poly_check # Проверяем получившуюся матрицу t_poly_check. - вывод матрицы G
# print (t_poly_check) # Выводим на экран получившуюся матрицу t_poly_check.
"""    [  1. ,   1. ,  -0.5],
       [  1. ,   2. ,  -2. ],
       [  1. ,   3. ,  -4.5],
       [  1. ,   4. ,  -8. ],
    G= [  1. ,   5. , -12.5],
       [  1. ,   6. , -18. ],
       [  1. ,   7. , -24.5],
       [  1. ,   8. , -32. ],
       [  1. ,   9. , -40.5],
       [  1. ,  10. , -50. ]
       """

'Обучение модели регрессии'
"""Находим параметры оценки мах правдопободия каждого параметра Ml2
то есть параметры которые максимизируют вероятность наблюдаемых данных при данной модели"""
# 회귀 모델 학습 Обучение модели регрессии
model = LinearRegression() # Создаётся объект LinearRegression() из библиотеки sklearn.linear_model
# y= ax1+bx2+c
model.fit(t_poly, y) # Метод .fit(X, y) обучает модель на данных

# 계수 출력 (-0.5a*t^2 + b*t + c 형태) Вывод коэффициентов в форме Уравнение регрессии: y = a * t² + b * t + c
m1, m2, m3 = model.intercept_, model.coef_[1], model.coef_[2] #Определение еоэффициентов модели
# model.coef_ — массив коэффициентов модели (например, линейной регрессии)
# coef_[2]   = m3 — расчет коэффициент при t^2
# coef_[1]   = m2 — расчет коэффициент при t
# model.intercept_ — свободный член (связан с c)
# intercept_ = m1 — константа (свободный член)
print(f"m1={m1:.4f}m")
print(f"m2={m2:.4f}m/s")
print(f"m3={m3:.4f}m/s^2")
# print(f"회귀식(Первая форма вывода уравнения): y = {m1:.4f} * t^2 + {m2:.4f} * t + {m3:.4f}") # Первая форма вывода уравнения?нужна ли она
# .4f означает округление до 4-х знаков после запятой.
#print('')
# Выводит пустую строку — для визуального разделения блоков

# y(t) = m_1 + m_2 t - (1/2)m_3 t^2
print(f"교재 regression model:") # Описание альтернативной формы представления уравнения — из учебника (교재).
print(f"y = {m1:.4f} + {m2:.4f} * t - (1/2) {m3/(-0.5):.4f} * t^2")
print('')
""" 
Получаем коэффициенты mL2 for every parameters m
m1=16.4083m
m2=96.9713m/s
m3=-4.7042m/s^2
교재 regression model:
y = 16.4083 + 96.9713 * t - (1/2) 9.4083 * t^2

"______________________________________________________________________________________"
"Построение графика Regression"
"""

# 예측값 계산
t_fit = np.linspace(1, 10, 100).reshape(-1, 1)
# Создаётся 100 точек t от 1 до 10 (включительно), равномерно распределённых
t_fit_poly = poly.transform(t_fit)
# Преобразуем t_fit в полиномиальные признаки с помощью ранее созданного трансформера poly, например t->[t^2,t]
y_fit = model.predict(t_fit_poly)
# Получаем предсказанные значения y от обученной модели для этих новых t-значений.y=m1t^2+m2t+c
# 시각화
plt.figure(figsize=(5, 3))  # (가로, 세로) 단위는 인치
plt.scatter(t, y, color='blue', s=10, marker='x', label='실제 데이터(real data)')
plt.plot(t_fit, y_fit, color='red', lw=0.5, label='2차 회귀 곡선 кривая квадратичной регрессии') # 2차 회귀 곡선-кривая квадратичной регрессии(или вторая степень регрессии)
plt.xlabel('t (s)')
plt.ylabel('y (m)')
plt.title('2차 다항 회귀 (Quadratic Regression) Квадратичная регрессия')
plt.legend()
plt.grid(True)
plt.show()

"""
"____________________________________________________"

"Example 2.2" 
"""
"""Goal: Построить линейную регрессию вида y(t)=m1+m2t-0.5m3^2
и оценить доверие к коэффициентам через ковариационную матрицу"""

# 1. 데이터 정의 DATA
t = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0])

# 2. 디자인 행렬 구성: [1, t, -0.5 * t^2] Создание матрицы вида G=[1,t,-0.5t^2]
G = np.hstack([       # горизонтальная (horizontal stack) конкатенация массивов объединяет массивы по колонкам
    np.ones_like(t),  # c (절편) свободный Создаёт массив той же формы, что и t, заполненный единицами
    t,                # b 계수   линейный
    -0.5 * t**2       # a 계수와 곱해질 항 с учетом T^2 и физической формулы
])

# 3. 선형 회귀 모델 (절편 직접 넣었으므로 fit_intercept=False)
# Обучение модели без автоматического интерсепта
model = LinearRegression(fit_intercept=False)
model.fit(G, y) # 
m_L2 = model.coef_ # нахождение Коэффициентов mL2
print("mL2: ", np.round(m_L2, 2)) # mL2:  [16.41m 96.97m/s 9.41m/s^2]
# 4. 알려진 오차 표준편차 Известная стандартная ошибка данных
sigma = 8
sigma_squared = sigma ** 2

# 5. 공분산 행렬 계산 Ковариационная матрица коэффициентов
GtG_inv = np.linalg.inv(G.T @ G) # Формула Cov[mL2]=(sigma^2)(G^T*G)^-1
cov_m_L2 = sigma_squared * GtG_inv # Формула расчета Ковариационной матрицы

# 6. 결과 출력
# print("추정된 회귀 계수 [m1, m2, m3]:", m_L2)
print("\n오차 분산 шумы σ² =", sigma_squared)
print("\n계수 공분산 행렬 коварианционная матрица (Cov[m_L2]):\n", np.round(cov_m_L2, 2))
""" Result
mL2:  [16.41 96.97  9.41]
오차 분산 шумы σ² = 64
계수 공분산 행렬 ковариацмонная матрица (Cov[m_L2]):
             [ 88.53 -33.6   -5.33]
 Cov[m_L2] = [-33.6   15.44   2.67]
             [ -5.33   2.67   0.48]
 
"""
"_________________________________________________________________________"
"""Goal: Посчитать 95% доверительные интервалы для каждого коэффициента регрессии
m1,m2,m3 используя
- Оценку коэффициента mL2
- Ковариационную матрицу(cov[mL2])
- Квантиль нормального распределения (z = 1.96)
"""
from scipy.stats import norm

# z 값 (정규분포 기준 95% 신뢰수준) 
#  Импорт квантиля нормального распределения
z = norm.ppf(0.975)  # 약 1.96
# ppf = percent point function (обратная функция к cdf)
# 𝑧 ≈ 1.96 z≈1.96 для 95% доверия (двустороннего)

# 신뢰구간 계산 Расчёт доверительных интервалов:
# Формула интервала: Coef Int = m+-z*se где SE - это стандартная ошибка
conf_intervals = []  # Инициализируем пустой список для хранения доверительных интервалов каждого коэффициента.
for i in range(len(m_L2)):  # Итерируем по индексам коэффициентов регрессии (у нас их 3: m1, m2, m3)
    se = np.sqrt(cov_m_L2[i, i])  # стандартная ошибка оценки 
    # (cov_m_L2[i, i]) - дисперсия конкретного параметра
    # se = np.sqrt() - вычисляем стандартную ошибку (SE) как корень из дисперсии
    lower = m_L2[i] - z * se      # нижняя граница
    upper = m_L2[i] + z * se      # верхняя граница
    conf_intervals.append((lower, upper)) # Сохраняем рассчитанный интервал в список
print("conf interval",conf_intervals)
"""
   Вывод доверительных интервалов для каждого коэффециента mL2
   [-2.0334, 34.8501]
   [89.2692, 104.6733]
   [8.0436, 10.7731]
"""
# 결과 출력 Красивый вывод с подписями 
labels = ['m_1', 'm_2', 'm_3']  # Названия коэффициентов m1,m2,m3
for i, (ci, est) in enumerate(zip(conf_intervals, m_L2)):
    print(f"{labels[i]} = {est:.4f},  95% 신뢰구간: ({ci[0]:.4f}, {ci[1]:.4f})")
    """
    m_1 = 16.4083,  95% 신뢰구간: (-2.0334, 34.8501)
    m_2 = 96.9713,  95% 신뢰구간: (89.2692, 104.6733)
    m_3 = 9.4083,  95% 신뢰구간: (8.0436, 10.7731)
    Interpretation: 
    Производим 
    - расчет доверительных интервалов используя квантиль нормального распределения Z
    - расчитываем стандартные ошибки 
    - используя расчет стандартных ошибок расчитвываем границу верхних и нижних интервалов 
    - производим расчем доверительных интервалов для каждого коэффициента
    - в полученных доверительных интервалах наши значения считаются верными 
    """

"""Goal: Проверить, насколько хорошо модель регрессии объясняет данные,
при заданной дисперсии ошибок 𝜎^2, используя:
- Хи-квадрат статистику: χ² = 
- сравнить её с критическим распределением χ² с соответствующими степенями свободы (df)
χ² (chi_squared) - насколько большая ошибка
df (степени своб.)	10 - число параметров
p-value	вероятность увидеть такую ошибку случайно
"""    
from scipy.stats import chi2 # Позволяет работать с распределением хи-квадрат:χ²

# 예측 및 잔차 Предсказание и остатки
y_pred = model.predict(G) # y_pred: предсказанные значения модели
residuals = y - y_pred    # residuals: остатки (разность между реальными и предсказанными)

# 카이제곱 통계량 계산: RSS / sigma^2 Выбор дисперсии и расчёт RSS(сумма квадратов ошибок)
sigma = 8
RSS = np.sum(residuals**2)
chi_squared = RSS / (sigma**2)

# 자유도 Степени свободы v=m-n
df = len(y) - G.shape[1]  # 10 - 3 = 7

# p-value 계산 (우측 누적 확률) Расчёт p-value
p_value = 1 - chi2.cdf(chi_squared, df=df)
# chi_squared-вероятность наблюдать меньшее или равное значение
# 1 - cdf — вероятность наблюдать больше → то есть наша p-value
# Если p-value < 0.05  p-value  0.05 → модель не объясняет данные хорошо (гипотеза об адекватности отвергается)

# 출력  Вывод
print(f"Chi-squared statistic: {chi_squared:.4f}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value:.4e}")
"Если p-value < 0.05  p-value  0.05 → модель не объясняет данные хорошо (гипотеза об адекватности отвергается)"
"""
χ²: 4.1810
Degrees of freedom(df): 7
p-value: 7.5871e-01    
"""
"_________________________________________________________________________"

"Example 2.2: Confidence ellipsoid"

"""Goal: Построить матрицу корреляций между коэффициентами модели: 𝑚1,𝑚2,𝑚3
основываясь на их ковариационной матрице
Построить эллипсоид доверия для коэффициентов регрессии m1,m2,m3   """
import pandas as pd

# 표준편차 벡터 (각 계수의 standard error)
# Стандартные ошибки для каждого коэффициента расчитывается как stand_error=sqrt(Var(m))
# Cov[mL2]=(sigma^2)(G^T*G)^-1
std_errors = np.sqrt(np.diag(cov_m_L2)) # Извлекаем диагональные элементы ковариационной матрицы
# np.diag(cov_m_L2) - извлекает диагональные элементы матрицы cov_m_L2
# np.sqrt - извлекает квадратный корень из каждого элемента
# std_errors = [sqrt(cov_m_L2[0,0]), sqrt(cov_m_L2[1,1]), sqrt(cov_m_L2[2,2])]

# 상관계수 행렬 계산 Формула корреляционной матрицы дано в учебнике
corr_matrix = cov_m_L2 / (std_errors[:, None] * std_errors[None, :]) # формулы ковариационной матрицы коэффициентов Corr[mi:mj]=(Cov[mL2])/(stdErrorsi*stdErrorsj)
# std_errors[:, None] - делает вектор столбцом
# std_errors[None, :] - делает вектор строкой
"→ их произведение даёт матрицу размером (3×3), пригодную для поэлементного деления всей cov_m_L2"
corr_df = pd.DataFrame(corr_matrix, index=labels, columns=labels)
# pd.DataFrame(...) превращает NumPy-массив в читабельную таблицу

# 결과 출력
labels = ['m_1', 'm_2', 'm_3']
print(std_errors.round(2))
print("계수 상관계수 행렬(Correlation coefficient matrix):")
print(corr_df.round(4))  # 소수점 4자리까지 보기 좋게 출력(округление до 4 знаков после запятой)
"""    m_1     m_2     m_3
m_1  1.0000 -0.9087 -0.8140
m_2 -0.9087  1.0000  0.9746
m_3 -0.8140  0.9746  1.0000      """
"""Interpretation:
Выведенная Кореляционная матрица параметров m1,m2,m3
- полученная из расчета стандартных ошибок для каждого m [9.41 3.93 0.7 ]
- полученная из формулы ковариационной матрицы коэффициентов Corr[m:m]=(Cov[mL2])/(stdErrorsi*stdErrorsj)
Полученаая матрица поазывает p ∈[-1,1] 1 = сильная полож. связь, 0 = нет связи, -1 = отриц. связь
Диагональ всегда = 1 -> каждый параметр полностью коррелирован сам с собой Если значения близки к ±1 — признаки скоррелированы
"""
"____________________________________________________________________________________________"
"Goal: Выполнить диагонолизацию(спектральное разложение)обратной ковариационной матрицы"
"""
- в каких направлениях параметры регрессии определены лучше/хуже через  геометрию параметрической неопределённости (через собственные значения/векторы)
- убедиться в корректности восстановления через спектральную формулу A=QΛQ^-1
- как геометрически выглядит доверительная область
"""
# 공분산 행렬의 역행렬 Inverse covvariotionate matrix Называется информационной матрицей (information matrix)
inv_cov = np.linalg.inv(cov_m_L2) 
# inv_cov — её обратная матрица Cov^-1

# 역행렬의 고유값 분해 (diagonalization) Собственное разложение через спектральную формулу A=QΛQ^-1
eigvals, eigvecs = np.linalg.eig(inv_cov)
# eigvals →Λ: массив собственных значений
# eigvecs →Q: матрица собственных векторов

# 고유값 오름차순 정렬 Сортировка по возрастанию собственных значений
idx = np.argsort(eigvals) # argsort - это 
eigvals_sorted = eigvals[idx]
eigvecs_sorted = eigvecs[:, idx]
# eigvecs_sorted — переставляем столбцы в том же порядке

# 정렬된 고유값/고유벡터로 역행렬 복원 Восстанавливаем исходную матрицу по формуле A=QΛQ^-1
reconstructed_sorted = eigvecs_sorted @ np.diag(eigvals_sorted) @ np.linalg.inv(eigvecs_sorted)

# 출력 (예쁘게 보기 위해 옵션 설정)
np.set_printoptions(precision=4, suppress=True)
print('')
print("역행렬[Исходную](Cov⁻¹):\n", inv_cov)
print('')
print("정렬된 고유값[Отсортированные собственные значения](Λ):\n", eigvals_sorted)
print('')
print("\n정렬된 고유벡터 행렬[Матрицу собственных векторов](Q):\n", eigvecs_sorted)
print('')
print("\nQ Λ Q⁻¹ 복원 결과 (정렬된 고유값 기준)[Реконструированную матрицу]:\n", reconstructed_sorted)
"""
Result
역행렬[Исходную](Cov⁻¹):
 [[  0.1562   0.8594  -3.0078]
 [  0.8594   6.0156 -23.6328]
 [ -3.0078 -23.6328  98.957 ]]
정렬된 고유값[Отсортированные собственные значения](Λ):
 [  0.0098   0.4046 104.7145]

정렬된 고유벡터 행렬[Матрицу собственных векторов](Q):
 [[-0.93    0.3663 -0.0299]
 [ 0.3629  0.9022 -0.233 ]
 [ 0.0584  0.2275  0.972 ]]

Q Λ Q⁻¹ 복원 결과 (정렬된 고유값 기준)[Реконструированную матрицу]:
 [[  0.1562   0.8594  -3.0078]
 [  0.8594   6.0156 -23.6328]
 [ -3.0078 -23.6328  98.957 ]]
 Interpretation:
 - Собственные векторы(eigvecsQ) дают главные направления неопределённости модели.
 - Собственные значения(eigvalsΛ) — интенсивность (информационная "чёткость") в этих направлениях.
 Используется для:
- построения доверительных эллипсоидов,
- PCA, SVD,
- байесовской аппроксимации.
 """
 
"Goals: Построить доверительный эллипсоид для параметров m=[m1,m2,m3]на основе ковариационной матрицы. Это визуализация неопределённости оценки." 
# 신뢰수준 (95%)에 해당하는 카이제곱 분위값 Значение квантиля распределения хи-квадрат, соответствующее доверительному уровню (95%)
p = 3  # 계수 개수 число параметров в модели (размерность пространства)
chi2_val = chi2.ppf(0.95, df=p) # 95%-квантиль распределения χ² с 3 степенями свободы
# Используется в многомерной статистике для построения доверительных областей

# 95% 신뢰 타원체의 반축 길이 (semiaxis) Длины полуосей (semiaxis) эллипсоида доверия 95%
semiaxes = np.sqrt(chi2_val / eigvals_sorted)
# eigvals_sorted — собственные значения обратной ковариационной матрицы



# 출력
np.set_printoptions(precision=4, suppress=True)
print('')
print("고유값 (오름차순)обственные значения 𝜆𝑖:", eigvals_sorted)
print("95% 신뢰수준 Chi² Квантиль χ² (граница эллипсоида) 값:", chi2_val)
print("신뢰 타원체의 반축 길이 (semiaxes)Полудлины осей эллипсоида вдоль собственных направлений:", semiaxes)
""" 
고유값 (오름차순)обственные значения 𝜆𝑖: [  0.0098   0.4046 104.7145]
95% 신뢰수준 Chi² Квантиль χ² (граница эллипсоида) 값: 7.814727903251176
신뢰 타원체의 반축 길이 (semiaxes)Полудлины осей эллипсоида вдоль собственных направлений: [28.2301  4.395   0.2732]
Interpretation:
Этот код рассчитывает размеры полуосей доверительного эллипсоида (на 95% доверительном уровне) в пространстве параметров регрессии m1,m2,m3
- Эллипсоид в пространстве m1,m2,m3 Он показывает, в каких направлениях модель наиболее/наименее уверена.
- Полуоси: показывают степень неопределённости вдоль главных направлений
- Длина оси
- Эллипсоид строится по собственным значениям обратной ковариационной матрицы.
"""
"____________________________________________________________________________________________________________________"

"Goals plot,vizualization"
import matplotlib.gridspec as gridspec

# 구면 생성
u = np.linspace(0, 2 * np.pi, 50)
v = np.linspace(0, np.pi, 50)
x = np.outer(np.cos(u), np.sin(v))
y = np.outer(np.sin(u), np.sin(v))
z = np.outer(np.ones_like(u), np.cos(v))
sphere = np.stack([x, y, z], axis=-1).reshape(-1, 3)

# 회전, 스케일, 이동
trans_axes = eigvecs_sorted @ np.diag(semiaxes)
ellipsoid = (trans_axes @ sphere.T).T + m_L2

# 투영
projections = [
    (ellipsoid[:, 0], ellipsoid[:, 1]),  # m1-m2
    (ellipsoid[:, 0], ellipsoid[:, 2]),  # m1-m3
    (ellipsoid[:, 1], ellipsoid[:, 2])   # m2-m3
]
labels = [('$m_1$', '$m_2$'), ('$m_1$', '$m_3$'), ('$m_2$', '$m_3$')]
lims = [[-50, 50, 85, 110], [-50, 50, 7, 12], [80, 120, 7, 12]]
colors = ['r', 'b', 'g']

# GridSpec으로 2:1:1:1 비율 subplot 배치
fig = plt.figure(figsize=(15, 6))
gs = gridspec.GridSpec(1, 4, width_ratios=[2, 1, 1, 1], figure=fig)

# 3D Plot
ax3d = fig.add_subplot(gs[0], projection='3d')
ax3d.scatter(ellipsoid[:, 1], ellipsoid[:, 0], np.full_like(ellipsoid[:, 2], 7),
             s=0.5, color='r', alpha=0.2, label='(m1,m2)')
ax3d.scatter(np.full_like(ellipsoid[:, 1], 80), ellipsoid[:, 0], ellipsoid[:, 2],
             s=0.5, color='b', alpha=0.2, label='(m1,m3)')
ax3d.scatter(ellipsoid[:, 1], np.full_like(ellipsoid[:, 0], -50), ellipsoid[:, 2],
             s=0.5, color='g', alpha=0.2, label='(m2,m3)')
ax3d.scatter(ellipsoid[:, 1], ellipsoid[:, 0], ellipsoid[:, 2],
             s=0.5, color='gray', alpha=0.05, label="Ellipsoid")

ax3d.set_ylabel('$m_1$ (m)')
ax3d.set_xlabel('$m_2$ (m/s)')
ax3d.set_zlabel('$m_3$ (m/s²)')
ax3d.set_ylim(-50, 50)
ax3d.set_xlim(110, 80)
ax3d.set_zlim(7, 12)
ax3d.view_init(elev=30, azim=115)
ax3d.set_box_aspect((1, 1, 1.7))
ax3d.legend(markerscale=10)

# 2D 평면 슬라이스
for i, (proj, (lx, ly), lim, color) in enumerate(zip(projections, labels, lims, colors), start=1):
    ax = fig.add_subplot(gs[i])
    ax.scatter(proj[0], proj[1], s=0.5, color=color, alpha=0.5)
    ax.set_xlabel(lx)
    ax.set_ylabel(ly)
    ax.set_xlim(lim[0], lim[1])
    ax.set_ylim(lim[2], lim[3])

plt.tight_layout()
plt.show()

""
# semiaxis length를 회귀좌표계로 변환
semiaxes_matrix = np.diag(semiaxes)
semiaxis_vectors = eigvecs_sorted @ semiaxes_matrix

# 출력
np.set_printoptions(precision=4, suppress=True)
print("회귀좌표계에서의 타원[Эллипс в координатах регрессии]:\n", semiaxis_vectors)
""" 
회귀좌표계에서의 타원[Эллипс в координатах регрессии]:
 [[-26.254    1.6101  -0.0082]
 [ 10.2445   3.9653  -0.0637]
 [  1.6488   1.       0.2655]]
Вектор, полученный преобразованием радиуса самой длинной оси эллипсоида в координаты регрессии, соответствует половине ширины ограничивающего прямоугольника (bounding box).
Следовательно, первая колонка матрицы 𝑃⋅diag(eigenvalues)
P⋅diag(eigenvalues) представляет собой половину доверительного интервала.
"""

# 신뢰구간 계산
se = semiaxis_vectors[:,0]
conf_intervals = []
for i in range(len(m_L2)):
    lower = m_L2[i] - abs(se[i])
    upper = m_L2[i] + abs(se[i])
    conf_intervals.append((lower, upper))

labels = ['m_1', 'm_2', 'm_3']
for i, (ci, est) in enumerate(zip(conf_intervals, m_L2)):
    print(f"{labels[i]} = {est:.4f},  95% 신뢰구간: ({ci[0]:.4f}, {ci[1]:.4f})")
""" 
m_1 = 16.4083,  95% 신뢰구간: (-9.8456, 42.6623)
m_2 = 96.9713,  95% 신뢰구간: (86.7267, 107.2158)
m_3 = 9.4083,  95% 신뢰구간: (7.7596, 11.0571)
"""