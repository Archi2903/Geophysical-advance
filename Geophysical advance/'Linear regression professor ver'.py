'Linear regression'
'Statistical aspects of leaat squares regression'
'Least squares regression is a method to find the best-fitting line through a set of points in a scatter plot.'

'блок настроек и импорта библиотек'
import numpy as np # для работы с массивами и матрицами
import matplotlib.pyplot as plt # для построения графиков
from sklearn.linear_model import LinearRegression # Используется для построения линейной регрессионной модели
from sklearn.preprocessing import PolynomialFeatures # для полиномиальной регрессии
 
import matplotlib.font_manager as fm # для работы с шрифтами
import matplotlib as mpl # для работы с графиками основной модуль, позволяет задавать глобальные настройки оформления графиков.

# 예: Windows 기본 한글 글꼴 중 하나 설정 (Malgun Gothic)
plt.rcParams['font.family'] = 'Malgun Gothic' #Установка шрифта Malgun Gothic как основного шрифта для всех графиков.
# Обеспечивает корректное отображение корейских символов на графиках (в противном случае могут отображаться квадратики или знаки вопроса).

# 음수 기호 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False #Устанавливает параметр, который позволяет корректно отображать минус («−») в осях графиках, даже если используется шрифт, который не поддерживает этот символ.
'________________________________________________________________________________________'

'DATA'
# Данные для линейной регрессии
t = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Входные данные, преобразованный в двумерный массив (через reshape)
# В машинном обучении большинство моделей требуют, чтобы входные данные были в виде двумерного массива — даже если есть только один признак.
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0])

'Создание полиномиальных признаков (второй степени)'
'Полиномы polynomial - необходима,чтобы модель могла описывать нелинейные зависимости между входом и выходом'
poly = PolynomialFeatures(degree=2) 
# Создаётся объект poly класса PolynomialFeatures с параметром degree=2, что означает, что мы хотим создать полиномиальные признаки второй степени.
# например, если у нас есть признак x, то полиномиальные признаки будут x и x^2.
# В результате мы получим два новых признака: x и x^2, которые будут использоваться в модели линейной регрессии.
t_poly = poly.fit_transform(t)
# fit-анализирует структуру данных (хотя тут это формальность) transform-преобразует входные данные t в полиномиальные признаки.
# В результате t_poly будет содержать два столбца: первый столбец - это t, а второй - это t^2.
'Проверка формулы полинома где G=[1, t, -1/2*t^2]'
t_poly_check = t_poly.copy() # Создаём копию t_poly, чтобы не изменять оригинальный массив.
t_poly_check[:, 2] *= -0.5 # Умножаем третий столбец (t^2) на -0.5, чтобы получить формулу G=[1, t, -1/2*t^2].
t_poly_check # Проверяем получившуюся матрицу t_poly_check.
# print (t_poly_check) # Выводим на экран получившуюся матрицу t_poly_check.
"""   [  1. ,   1. ,  -0.5],
       [  1. ,   2. ,  -2. ],
       [  1. ,   3. ,  -4.5],
       [  1. ,   4. ,  -8. ],
       [  1. ,   5. , -12.5],
       [  1. ,   6. , -18. ],
       [  1. ,   7. , -24.5],
       [  1. ,   8. , -32. ],
       [  1. ,   9. , -40.5],
       [  1. ,  10. , -50. ]"""

'Обучение модели регрессии'
"""Находим параметры оценки мах правдопободия каждого параметра Ml2
то есть параметры которые максимизируют вероятность наблюдаемых данных при данной модели"""
# 회귀 모델 학습 Обучение модели регрессии
#model = LinearRegression() # Создаётся объект LinearRegression() из библиотеки sklearn.linear_model
# y= ax1+bx2+c
#model.fit(t_poly, y) # Метод .fit(X, y) обучает модель на данных

# 계수 출력 (-0.5a*t^2 + b*t + c 형태) Вывод коэффициентов в форме Уравнение регрессии: y = a * t² + b * t + c
#m1, m2, m3 = model.coef_[2], model.coef_[1], model.intercept_ #Определение еоэффициентов модели
# model.coef_ — массив коэффициентов модели (например, линейной регрессии)
# coef_[2] = m1 — коэффициент при t^2
# coef_[1] = m2 — коэффициент при t
# model.intercept_ — свободный член (связан с c)
# intercept_ = m3 — константа (свободный член)
#print(f"회귀식: y = {m1:.4f} * t^2 + {m2:.4f} * t + {m3:.4f}") # Первая форма вывода уравнения
# .4f означает округление до 4-х знаков после запятой.
#print('')
# Выводит пустую строку — для визуального разделения блоков

# y(t) = m_1 + m_2 t - (1/2)m_3 t^2
#print(f"교재 regression model:") # Описание альтернативной формы представления уравнения — из учебника (교재).
#print(f"y = {m3:.4f} + {m2:.4f} * t - (1/2) {m1/(-0.5):.4f} * t^2")
"______________________________________________________________________________________"
"Построение графика Regression"
"""# 예측값 계산
t_fit = np.linspace(1, 10, 100).reshape(-1, 1)
# Создаётся 100 точек t от 1 до 10 (включительно), равномерно распределённых
t_fit_poly = poly.transform(t_fit)
# Преобразуем t_fit в полиномиальные признаки с помощью ранее созданного трансформера poly, например t->[t^2,t]
y_fit = model.predict(t_fit_poly)
# Получаем предсказанные значения y от обученной модели для этих новых t-значений.y=m1t^2+m2t+c
# 시각화
plt.figure(figsize=(5, 3))  # (가로, 세로) 단위는 인치
plt.scatter(t, y, color='blue', s=10, marker='x', label='실제 데이터')
plt.plot(t_fit, y_fit, color='red', lw=0.5, label='2차 회귀 곡선')
plt.xlabel('t (s)')
plt.ylabel('y (m)')
plt.title('2차 다항 회귀 (Quadratic Regression)')
plt.legend()
plt.grid(True)
plt.show()"""
"____________________________________________________"

"Example 2.2"
"""Goal: Построить линейную регрессию вида y(t)=m1+m2t-0.5m3^2
и оценить доверие к коэффициентам через ковариационную матрицу"""

# 1. 데이터 정의
t = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0])

# 2. 디자인 행렬 구성: [1, t, -0.5 * t^2] Создание матрицы вида G=[1,t,-0.5t^2]
G = np.hstack([       # горизонтальная (horizontal stack) конкатенация массивов объединяет массивы по колонкам
    np.ones_like(t),  # c (절편) свободный Создаёт массив той же формы, что и t, заполненный единицами
    t,                # b 계수   линейный
    -0.5 * t**2       # a 계수와 곱해질 항 с учетом T^2 и физической формулы
])

# 3. 선형 회귀 모델 (절편 직접 넣었으므로 fit_intercept=False)
# Обучение модели без автоматического интерсепта
model = LinearRegression(fit_intercept=False)
model.fit(G, y)
m_L2 = model.coef_

# 4. 알려진 오차 표준편차 Известная стандартная ошибка данных
sigma = 8
sigma_squared = sigma ** 2

# 5. 공분산 행렬 계산 Ковариационная матрица коэффициентов
GtG_inv = np.linalg.inv(G.T @ G) # Формула Cov[mL2]=(sigma^2)(G^T*G)^-1
cov_m_L2 = sigma_squared * GtG_inv

# 6. 결과 출력
print("추정된 회귀 계수 [m3, m2, m1]:", m_L2)
print("\n오차 분산 σ² =", sigma_squared)
print("\n계수 공분산 행렬 (Cov[m_L2]):\n", cov_m_L2)

