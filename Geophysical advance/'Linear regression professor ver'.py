'Linear regression'
'Statistical aspects of leaat squares regression'
'Least squares regression is a method to find the best-fitting line through a set of points in a scatter plot.'

'блок настроек и импорта библиотек'
import numpy as np # для работы с массивами и матрицами
import matplotlib.pyplot as plt # для построения графиков
from sklearn.linear_model import LinearRegression # Используется для построения линейной регрессионной модели
from sklearn.preprocessing import PolynomialFeatures # для полиномиальной регрессии
 
import matplotlib.font_manager as fm # для работы с шрифтами
import matplotlib as mpl # для работы с графиками основной модуль, позволяет задавать глобальные настройки оформления графиков.

# 예: Windows 기본 한글 글꼴 중 하나 설정 (Malgun Gothic)
plt.rcParams['font.family'] = 'Malgun Gothic' #Установка шрифта Malgun Gothic как основного шрифта для всех графиков.
# Обеспечивает корректное отображение корейских символов на графиках (в противном случае могут отображаться квадратики или знаки вопроса).

# 음수 기호 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False #Устанавливает параметр, который позволяет корректно отображать минус («−») в осях графиках, даже если используется шрифт, который не поддерживает этот символ.
'________________________________________________________________________________________'

'DATA'
# Данные для линейной регрессии
t = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Входные данные, преобразованный в двумерный массив (через reshape)
# В машинном обучении большинство моделей требуют, чтобы входные данные были в виде двумерного массива — даже если есть только один признак.
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0])

'Создание полиномиальных признаков (второй степени)'
'Полиномы polynomial - необходима,чтобы модель могла описывать нелинейные зависимости между входом и выходом'
poly = PolynomialFeatures(degree=2) 
# Создаётся объект poly класса PolynomialFeatures с параметром degree=2, что означает, что мы хотим создать полиномиальные признаки второй степени.
# например, если у нас есть признак x, то полиномиальные признаки будут x и x^2.
# В результате мы получим два новых признака: x и x^2, которые будут использоваться в модели линейной регрессии.
t_poly = poly.fit_transform(t)
# fit-анализирует структуру данных (хотя тут это формальность) transform-преобразует входные данные t в полиномиальные признаки.
# В результате t_poly будет содержать два столбца: первый столбец - это t, а второй - это t^2.
'Проверка формулы полинома где G=[1, t, -1/2*t^2]'
t_poly_check = t_poly.copy() # Создаём копию t_poly, чтобы не изменять оригинальный массив.
t_poly_check[:, 2] *= -0.5 # Умножаем третий столбец (t^2) на -0.5, чтобы получить формулу G=[1, t, -1/2*t^2].
t_poly_check # Проверяем получившуюся матрицу t_poly_check.
# print (t_poly_check) # Выводим на экран получившуюся матрицу t_poly_check.
"""   [  1. ,   1. ,  -0.5],
       [  1. ,   2. ,  -2. ],
       [  1. ,   3. ,  -4.5],
       [  1. ,   4. ,  -8. ],
       [  1. ,   5. , -12.5],
       [  1. ,   6. , -18. ],
       [  1. ,   7. , -24.5],
       [  1. ,   8. , -32. ],
       [  1. ,   9. , -40.5],
       [  1. ,  10. , -50. ]"""

'Обучение модели регрессии'
"""Находим параметры оценки мах правдопободия каждого параметра Ml2
то есть параметры которые максимизируют вероятность наблюдаемых данных при данной модели"""
# 회귀 모델 학습 Обучение модели регрессии
#model = LinearRegression() # Создаётся объект LinearRegression() из библиотеки sklearn.linear_model
# y= ax1+bx2+c
#model.fit(t_poly, y) # Метод .fit(X, y) обучает модель на данных

# 계수 출력 (-0.5a*t^2 + b*t + c 형태) Вывод коэффициентов в форме Уравнение регрессии: y = a * t² + b * t + c
#m1, m2, m3 = model.coef_[2], model.coef_[1], model.intercept_ #Определение еоэффициентов модели
# model.coef_ — массив коэффициентов модели (например, линейной регрессии)
# coef_[2] = m1 — коэффициент при t^2
# coef_[1] = m2 — коэффициент при t
# model.intercept_ — свободный член (связан с c)
# intercept_ = m3 — константа (свободный член)
#print(f"회귀식: y = {m1:.4f} * t^2 + {m2:.4f} * t + {m3:.4f}") # Первая форма вывода уравнения
# .4f означает округление до 4-х знаков после запятой.
#print('')
# Выводит пустую строку — для визуального разделения блоков

# y(t) = m_1 + m_2 t - (1/2)m_3 t^2
#print(f"교재 regression model:") # Описание альтернативной формы представления уравнения — из учебника (교재).
#print(f"y = {m3:.4f} + {m2:.4f} * t - (1/2) {m1/(-0.5):.4f} * t^2")
"______________________________________________________________________________________"
"Построение графика Regression"
"""# 예측값 계산
t_fit = np.linspace(1, 10, 100).reshape(-1, 1)
# Создаётся 100 точек t от 1 до 10 (включительно), равномерно распределённых
t_fit_poly = poly.transform(t_fit)
# Преобразуем t_fit в полиномиальные признаки с помощью ранее созданного трансформера poly, например t->[t^2,t]
y_fit = model.predict(t_fit_poly)
# Получаем предсказанные значения y от обученной модели для этих новых t-значений.y=m1t^2+m2t+c
# 시각화
plt.figure(figsize=(5, 3))  # (가로, 세로) 단위는 인치
plt.scatter(t, y, color='blue', s=10, marker='x', label='실제 데이터')
plt.plot(t_fit, y_fit, color='red', lw=0.5, label='2차 회귀 곡선')
plt.xlabel('t (s)')
plt.ylabel('y (m)')
plt.title('2차 다항 회귀 (Quadratic Regression)')
plt.legend()
plt.grid(True)
plt.show()"""
"____________________________________________________"

"Example 2.2"
"""Goal: Построить линейную регрессию вида y(t)=m1+m2t-0.5m3^2
и оценить доверие к коэффициентам через ковариационную матрицу"""

# 1. 데이터 정의
t = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)
y = np.array([109.4, 187.5, 267.5, 331.9, 386.1, 428.4, 452.2, 498.1, 512.3, 513.0])

# 2. 디자인 행렬 구성: [1, t, -0.5 * t^2] Создание матрицы вида G=[1,t,-0.5t^2]
G = np.hstack([       # горизонтальная (horizontal stack) конкатенация массивов объединяет массивы по колонкам
    np.ones_like(t),  # c (절편) свободный Создаёт массив той же формы, что и t, заполненный единицами
    t,                # b 계수   линейный
    -0.5 * t**2       # a 계수와 곱해질 항 с учетом T^2 и физической формулы
])

# 3. 선형 회귀 모델 (절편 직접 넣었으므로 fit_intercept=False)
# Обучение модели без автоматического интерсепта
model = LinearRegression(fit_intercept=False)
model.fit(G, y)
m_L2 = model.coef_

# 4. 알려진 오차 표준편차 Известная стандартная ошибка данных
sigma = 8
sigma_squared = sigma ** 2

# 5. 공분산 행렬 계산 Ковариационная матрица коэффициентов
GtG_inv = np.linalg.inv(G.T @ G) # Формула Cov[mL2]=(sigma^2)(G^T*G)^-1
cov_m_L2 = sigma_squared * GtG_inv

# 6. 결과 출력
print("추정된 회귀 계수 [m3, m2, m1]:", m_L2)
print("\n오차 분산 σ² =", sigma_squared)
print("\n계수 공분산 행렬 (Cov[m_L2]):\n", cov_m_L2)
""" Result
[m3, m2, m1]: [16.40833333 96.97128788  9.40833333]

오차 분산 σ² = 64

계수 공분산 행렬 (Cov[m_L2]):
 [[ 88.53333333 -33.6         -5.33333333]
 [-33.6         15.44242424   2.66666667]
 [ -5.33333333   2.66666667   0.48484848]]
"""
"_________________________________________________________________________"
"""Goal: Посчитать 95% доверительные интервалы для каждого коэффициента регрессии
m1,m2,m3 используя
- Оценку коэффициента mL2
- Ковариационную матрицу(cov[mL2])
- Квантиль нормального распределения (z = 1.96)
"""
from scipy.stats import norm

# z 값 (정규분포 기준 95% 신뢰수준) 
#  Импорт квантиля нормального распределения
z = norm.ppf(0.975)  # 약 1.96
# ppf = percent point function (обратная функция к cdf)
# 𝑧 ≈ 1.96 z≈1.96 для 95% доверия (двустороннего)

# 신뢰구간 계산 Расчёт доверительных интервалов:
# Формула интервала: Coef Int = m+-z*se
conf_intervals = []
for i in range(len(m_L2)):
    se = np.sqrt(cov_m_L2[i, i])  # стандартная ошибка оценки
    lower = m_L2[i] - z * se      # нижняя граница
    upper = m_L2[i] + z * se      # верхняя граница
    conf_intervals.append((lower, upper))

# 결과 출력
labels = ['m_1', 'm_2', 'm_3']  # Названия коэффициентов m1,m2,m3
for i, (ci, est) in enumerate(zip(conf_intervals, m_L2)):
    print(f"{labels[i]} = {est:.4f},  95% 신뢰구간: ({ci[0]:.4f}, {ci[1]:.4f})")
    """
    m_1 = 16.4083,  95% 신뢰구간: (-2.0334, 34.8501)
    m_2 = 96.9713,  95% 신뢰구간: (89.2692, 104.6733)
    m_3 = 9.4083,  95% 신뢰구간: (8.0436, 10.7731)
    """

"""Goal: Проверить, насколько хорошо модель регрессии объясняет данные,
при заданной дисперсии ошибок 𝜎^2, используя:
- Хи-квадрат статистику: χ² = 
- сравнить её с критическим распределением χ² с соответствующими степенями свободы (df)
χ² (chi_squared) - насколько большая ошибка
df (степени своб.)	10 - число параметров
p-value	вероятность увидеть такую ошибку случайно
"""    
from scipy.stats import chi2 # Позволяет работать с распределением хи-квадрат:χ²

# 예측 및 잔차 Предсказание и остатки
y_pred = model.predict(G) # y_pred: предсказанные значения модели
residuals = y - y_pred    # residuals: остатки (разность между реальными и предсказанными)

# 카이제곱 통계량 계산: RSS / sigma^2 Выбор дисперсии и расчёт RSS(сумма квадратов ошибок)
sigma = 8
RSS = np.sum(residuals**2)
chi_squared = RSS / (sigma**2)

# 자유도 Степени свободы v=m-n
df = len(y) - G.shape[1]  # 10 - 3 = 7

# p-value 계산 (우측 누적 확률) Расчёт p-value
p_value = 1 - chi2.cdf(chi_squared, df=df)
# chi_squared-вероятность наблюдать меньшее или равное значение
# 1 - cdf — вероятность наблюдать больше → то есть наша p-value
# Если p-value < 0.05  p-value  0.05 → модель не объясняет данные хорошо (гипотеза об адекватности отвергается)

# 출력  Вывод
print(f"Chi-squared statistic: {chi_squared:.4f}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value:.4e}")
"Если p-value < 0.05  p-value  0.05 → модель не объясняет данные хорошо (гипотеза об адекватности отвергается)"
"""
χ²: 4.1810
Degrees of freedom(df): 7
p-value: 7.5871e-01    
"""
"_________________________________________________________________________"
"Example 2.2: Confidence ellipsoid"
